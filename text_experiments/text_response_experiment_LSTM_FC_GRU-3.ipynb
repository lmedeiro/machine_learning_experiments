{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-rc1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)  # Ensure that we're using \"2.0.0-rc1\"\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as k\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as metrics\n",
    "import pdb\n",
    "from pdb import set_trace as bp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: \n",
    "This notebook builds on [this tensorflow tutorial](https://www.tensorflow.org/tutorials/text/text_generation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mmodels\u001b[0m/\r\n",
      "nyc_stock_exchange.txt\r\n",
      "principles_of_plitical_economy.txt\r\n",
      "stock_exchange_from_within.txt\r\n",
      "stock_exchange.txt\r\n",
      "text_response_experiment_0.ipynb\r\n",
      "text_response_experiment_LSTM_FC_GRU-2.ipynb\r\n",
      "text_response_experiment_LSTM_FC_GRU-3.ipynb\r\n",
      "text_response_experiment_LSTM_FC_GRU.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Category</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>Quote</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dr. Seuss</td>\n",
       "      <td>life</td>\n",
       "      <td>0.155666</td>\n",
       "      <td>Don't cry because it's over, smile because it ...</td>\n",
       "      <td>[attributed-no-source, cry, crying, experience...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dr. Seuss</td>\n",
       "      <td>happiness</td>\n",
       "      <td>0.155666</td>\n",
       "      <td>Don't cry because it's over, smile because it ...</td>\n",
       "      <td>[attributed-no-source, cry, crying, experience...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Marilyn Monroe</td>\n",
       "      <td>love</td>\n",
       "      <td>0.129122</td>\n",
       "      <td>I'm selfish, impatient and a little insecure. ...</td>\n",
       "      <td>[attributed-no-source, best, life, love, mista...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Marilyn Monroe</td>\n",
       "      <td>life</td>\n",
       "      <td>0.129122</td>\n",
       "      <td>I'm selfish, impatient and a little insecure. ...</td>\n",
       "      <td>[attributed-no-source, best, life, love, mista...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Marilyn Monroe</td>\n",
       "      <td>truth</td>\n",
       "      <td>0.129122</td>\n",
       "      <td>I'm selfish, impatient and a little insecure. ...</td>\n",
       "      <td>[attributed-no-source, best, life, love, mista...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Oscar Wilde</td>\n",
       "      <td>inspiration</td>\n",
       "      <td>0.113223</td>\n",
       "      <td>Be yourself; everyone else is already taken.</td>\n",
       "      <td>[attributed-no-source, be-yourself, honesty, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Albert Einstein</td>\n",
       "      <td>humor</td>\n",
       "      <td>0.103127</td>\n",
       "      <td>Two things are infinite: the universe and huma...</td>\n",
       "      <td>[attributed-no-source, human-nature, humor, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Albert Einstein</td>\n",
       "      <td>philosophy</td>\n",
       "      <td>0.103127</td>\n",
       "      <td>Two things are infinite: the universe and huma...</td>\n",
       "      <td>[attributed-no-source, human-nature, humor, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Albert Einstein</td>\n",
       "      <td>science</td>\n",
       "      <td>0.103127</td>\n",
       "      <td>Two things are infinite: the universe and huma...</td>\n",
       "      <td>[attributed-no-source, human-nature, humor, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bernard M. Baruch</td>\n",
       "      <td></td>\n",
       "      <td>0.101890</td>\n",
       "      <td>Be who you are and say what you feel, because ...</td>\n",
       "      <td>[ataraxy, be-yourself, confidence, fitting-in,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>William W. Purkey</td>\n",
       "      <td>love</td>\n",
       "      <td>0.100056</td>\n",
       "      <td>You've gotta dance like there's nobody watchin...</td>\n",
       "      <td>[dance, heaven, hurt, inspirational, life, lov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>William W. Purkey</td>\n",
       "      <td>life</td>\n",
       "      <td>0.100056</td>\n",
       "      <td>You've gotta dance like there's nobody watchin...</td>\n",
       "      <td>[dance, heaven, hurt, inspirational, life, lov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>William W. Purkey</td>\n",
       "      <td>inspiration</td>\n",
       "      <td>0.100056</td>\n",
       "      <td>You've gotta dance like there's nobody watchin...</td>\n",
       "      <td>[dance, heaven, hurt, inspirational, life, lov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Dr. Seuss</td>\n",
       "      <td>love</td>\n",
       "      <td>0.095724</td>\n",
       "      <td>You know you're in love when you can't fall as...</td>\n",
       "      <td>[attributed-no-source, dreams, love, reality, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Marcus Tullius Cicero</td>\n",
       "      <td>soul</td>\n",
       "      <td>0.095375</td>\n",
       "      <td>A room without books is like a body without a ...</td>\n",
       "      <td>[attributed-no-source, books, simile, soul ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Marcus Tullius Cicero</td>\n",
       "      <td>books</td>\n",
       "      <td>0.095375</td>\n",
       "      <td>A room without books is like a body without a ...</td>\n",
       "      <td>[attributed-no-source, books, simile, soul ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Frank Zappa</td>\n",
       "      <td>humor</td>\n",
       "      <td>0.095068</td>\n",
       "      <td>So many books, so little time.</td>\n",
       "      <td>[books, humor ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Frank Zappa</td>\n",
       "      <td>books</td>\n",
       "      <td>0.095068</td>\n",
       "      <td>So many books, so little time.</td>\n",
       "      <td>[books, humor ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Mae West</td>\n",
       "      <td>life</td>\n",
       "      <td>0.087721</td>\n",
       "      <td>You only live once, but if you do it right, on...</td>\n",
       "      <td>[humor, life ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Mae West</td>\n",
       "      <td>humor</td>\n",
       "      <td>0.087721</td>\n",
       "      <td>You only live once, but if you do it right, on...</td>\n",
       "      <td>[humor, life ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Mahatma Gandhi</td>\n",
       "      <td>inspiration</td>\n",
       "      <td>0.085248</td>\n",
       "      <td>Be the change that you wish to see in the world.</td>\n",
       "      <td>[action, change, inspirational, philosophy, wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Mahatma Gandhi</td>\n",
       "      <td>philosophy</td>\n",
       "      <td>0.085248</td>\n",
       "      <td>Be the change that you wish to see in the world.</td>\n",
       "      <td>[action, change, inspirational, philosophy, wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Robert Frost</td>\n",
       "      <td>life</td>\n",
       "      <td>0.081176</td>\n",
       "      <td>In three words I can sum up everything I've le...</td>\n",
       "      <td>[life ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>C.S. Lewis,  The Four Loves</td>\n",
       "      <td></td>\n",
       "      <td>0.073322</td>\n",
       "      <td>Friendship ... is born at the moment when one ...</td>\n",
       "      <td>[friendship ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>J.K. Rowling,  Harry Potter and the Goblet of ...</td>\n",
       "      <td></td>\n",
       "      <td>0.072186</td>\n",
       "      <td>If you want to know what a man's like, take a ...</td>\n",
       "      <td>[from-charles-bayard-miliken, misattributed-j-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Albert Camus</td>\n",
       "      <td></td>\n",
       "      <td>0.071550</td>\n",
       "      <td>Don’t walk in front of me… I may not followDon...</td>\n",
       "      <td>[friends, friendship, misattributed-albert-cam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Eleanor Roosevelt,  This is My Story</td>\n",
       "      <td>inspiration</td>\n",
       "      <td>0.069065</td>\n",
       "      <td>No one can make you feel inferior without your...</td>\n",
       "      <td>[confidence, inspirational, wisdom ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Eleanor Roosevelt,  This is My Story</td>\n",
       "      <td>wisdom</td>\n",
       "      <td>0.069065</td>\n",
       "      <td>No one can make you feel inferior without your...</td>\n",
       "      <td>[confidence, inspirational, wisdom ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>truth</td>\n",
       "      <td>0.067952</td>\n",
       "      <td>If you tell the truth, you don't have to remem...</td>\n",
       "      <td>[lies, lying, memory, truth ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Elbert Hubbard</td>\n",
       "      <td>love</td>\n",
       "      <td>0.066100</td>\n",
       "      <td>A friend is someone who knows all about you an...</td>\n",
       "      <td>[friend, friendship, knowledge, love ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48361</th>\n",
       "      <td>Boethius</td>\n",
       "      <td>love</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Who would give a law to lovers? Love is unto i...</td>\n",
       "      <td>[Law, Lovers, Higher]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48362</th>\n",
       "      <td>Mortimer Adler</td>\n",
       "      <td>love</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Love can be unselfish, in the sense of being b...</td>\n",
       "      <td>[Selfless, Unselfish]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48363</th>\n",
       "      <td>Jean Racine</td>\n",
       "      <td>love</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>The quarrels of lovers are the renewal of love.</td>\n",
       "      <td>[Lovers, Renewal, Quarrels]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48364</th>\n",
       "      <td>Theodor Adorno</td>\n",
       "      <td>love</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Love you will find only where you may show you...</td>\n",
       "      <td>[Strength, Weak, Provoking]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48365</th>\n",
       "      <td>Tom Robbins</td>\n",
       "      <td>love</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>The highest function of love is that it makes ...</td>\n",
       "      <td>[Unique, Loved, Function]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48366</th>\n",
       "      <td>Joseph Campbell</td>\n",
       "      <td>friendship</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Love is a friendship set to music.</td>\n",
       "      <td>[Friendship, Music]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48367</th>\n",
       "      <td>Joseph Campbell</td>\n",
       "      <td>love</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Love is a friendship set to music.</td>\n",
       "      <td>[Friendship, Music]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48368</th>\n",
       "      <td>Stendhal</td>\n",
       "      <td>hope</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>A very small degree of hope is sufficient to c...</td>\n",
       "      <td>[Hope, Small, Birth]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48369</th>\n",
       "      <td>Stendhal</td>\n",
       "      <td>love</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>A very small degree of hope is sufficient to c...</td>\n",
       "      <td>[Hope, Small, Birth]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48370</th>\n",
       "      <td>Patti Smith</td>\n",
       "      <td>arts</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>In art and dream may you proceed with abandon....</td>\n",
       "      <td>[Life, Balance, Dream]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48371</th>\n",
       "      <td>Thornton Wilder</td>\n",
       "      <td>arts</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>I regard the theatre as the greatest of all ar...</td>\n",
       "      <td>[Theatre, Greatest, Share]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48372</th>\n",
       "      <td>William Morris</td>\n",
       "      <td>arts</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>So long as the system of competition in the pr...</td>\n",
       "      <td>[Life, Arts, Die]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48373</th>\n",
       "      <td>Nelson Mandela</td>\n",
       "      <td>arts</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>I started to make a study of the art of war an...</td>\n",
       "      <td>[War, Training, Fight]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48374</th>\n",
       "      <td>Samuel Johnson</td>\n",
       "      <td>arts</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>There is nothing, Sir, too little for so littl...</td>\n",
       "      <td>[Happiness, Great, Studying]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48375</th>\n",
       "      <td>Bob Riley</td>\n",
       "      <td>success</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>No skill shapes a child's future success in sc...</td>\n",
       "      <td>[Life, Future, Child]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48376</th>\n",
       "      <td>Jane Rule</td>\n",
       "      <td>success</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>My private measure of success is daily. If thi...</td>\n",
       "      <td>[Life, Daily, Balance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48377</th>\n",
       "      <td>Anne Rice</td>\n",
       "      <td>success</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>First-person narrators is the way I know how t...</td>\n",
       "      <td>[Power, Chance, Book]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48378</th>\n",
       "      <td>Cameron Diaz</td>\n",
       "      <td>success</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>I haven't deliberately set out to play the blo...</td>\n",
       "      <td>[Time, Movies, Beginning]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48379</th>\n",
       "      <td>Shigeru Miyamoto</td>\n",
       "      <td>success</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>I think that the entertainment industry itself...</td>\n",
       "      <td>[Time, History, Start]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48380</th>\n",
       "      <td>Philip Treacy</td>\n",
       "      <td>success</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>The success of a hat definitely lies with bala...</td>\n",
       "      <td>[Rules, Face, Listen]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48381</th>\n",
       "      <td>Amy Tan</td>\n",
       "      <td>hope</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>I think I've always been somebody, since the d...</td>\n",
       "      <td>[Hope, Failure, Father]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48382</th>\n",
       "      <td>Amy Tan</td>\n",
       "      <td>success</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>I think I've always been somebody, since the d...</td>\n",
       "      <td>[Hope, Failure, Father]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48383</th>\n",
       "      <td>Mike Nichols</td>\n",
       "      <td>humor</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>I think the main thing about comedy and humor ...</td>\n",
       "      <td>[Comedy, Impossible, Define]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48384</th>\n",
       "      <td>Herbert Beerbohm Tree</td>\n",
       "      <td>humor</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Never say a humorous thing to a man who does n...</td>\n",
       "      <td>[Against, Evidence, Humorous]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48385</th>\n",
       "      <td>Ted Danson</td>\n",
       "      <td>humor</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Humor can bring people under the tent. And a g...</td>\n",
       "      <td>[Good, Bring, Serious]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48386</th>\n",
       "      <td>Jason Mraz</td>\n",
       "      <td>humor</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>In Buddhism, they say attachment to anything o...</td>\n",
       "      <td>[Suffering, Laugh, Stage]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48387</th>\n",
       "      <td>Beck</td>\n",
       "      <td>humor</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>I love British humor. It's just so - surreal.</td>\n",
       "      <td>[Love, British, Surreal]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48388</th>\n",
       "      <td>Daryl Hall</td>\n",
       "      <td>humor</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>I've got a sense of humor. I'm a funny guy.</td>\n",
       "      <td>[Funny, Guy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48389</th>\n",
       "      <td>Lynda Barry</td>\n",
       "      <td>humor</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Humor is such a wonderful thing, helping you r...</td>\n",
       "      <td>[Time, Beautiful, Fool]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48390</th>\n",
       "      <td>Pratik Shelar</td>\n",
       "      <td>inspiration</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>Life Is Full of Obstacles, Stumble Upon !!</td>\n",
       "      <td>[inspirational-quotes ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48391 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Author     Category  \\\n",
       "0                                              Dr. Seuss         life   \n",
       "1                                              Dr. Seuss    happiness   \n",
       "2                                         Marilyn Monroe         love   \n",
       "3                                         Marilyn Monroe         life   \n",
       "4                                         Marilyn Monroe        truth   \n",
       "5                                            Oscar Wilde  inspiration   \n",
       "6                                        Albert Einstein        humor   \n",
       "7                                        Albert Einstein   philosophy   \n",
       "8                                        Albert Einstein      science   \n",
       "9                                      Bernard M. Baruch                \n",
       "10                                     William W. Purkey         love   \n",
       "11                                     William W. Purkey         life   \n",
       "12                                     William W. Purkey  inspiration   \n",
       "13                                             Dr. Seuss         love   \n",
       "14                                 Marcus Tullius Cicero         soul   \n",
       "15                                 Marcus Tullius Cicero        books   \n",
       "16                                           Frank Zappa        humor   \n",
       "17                                           Frank Zappa        books   \n",
       "18                                              Mae West         life   \n",
       "19                                              Mae West        humor   \n",
       "20                                        Mahatma Gandhi  inspiration   \n",
       "21                                        Mahatma Gandhi   philosophy   \n",
       "22                                          Robert Frost         life   \n",
       "23                           C.S. Lewis,  The Four Loves                \n",
       "24     J.K. Rowling,  Harry Potter and the Goblet of ...                \n",
       "25                                          Albert Camus                \n",
       "26                  Eleanor Roosevelt,  This is My Story  inspiration   \n",
       "27                  Eleanor Roosevelt,  This is My Story       wisdom   \n",
       "28                                            Mark Twain        truth   \n",
       "29                                        Elbert Hubbard         love   \n",
       "...                                                  ...          ...   \n",
       "48361                                           Boethius         love   \n",
       "48362                                     Mortimer Adler         love   \n",
       "48363                                        Jean Racine         love   \n",
       "48364                                     Theodor Adorno         love   \n",
       "48365                                        Tom Robbins         love   \n",
       "48366                                    Joseph Campbell   friendship   \n",
       "48367                                    Joseph Campbell         love   \n",
       "48368                                           Stendhal         hope   \n",
       "48369                                           Stendhal         love   \n",
       "48370                                        Patti Smith         arts   \n",
       "48371                                    Thornton Wilder         arts   \n",
       "48372                                     William Morris         arts   \n",
       "48373                                     Nelson Mandela         arts   \n",
       "48374                                     Samuel Johnson         arts   \n",
       "48375                                          Bob Riley      success   \n",
       "48376                                          Jane Rule      success   \n",
       "48377                                          Anne Rice      success   \n",
       "48378                                       Cameron Diaz      success   \n",
       "48379                                   Shigeru Miyamoto      success   \n",
       "48380                                      Philip Treacy      success   \n",
       "48381                                            Amy Tan         hope   \n",
       "48382                                            Amy Tan      success   \n",
       "48383                                       Mike Nichols        humor   \n",
       "48384                              Herbert Beerbohm Tree        humor   \n",
       "48385                                         Ted Danson        humor   \n",
       "48386                                         Jason Mraz        humor   \n",
       "48387                                               Beck        humor   \n",
       "48388                                         Daryl Hall        humor   \n",
       "48389                                        Lynda Barry        humor   \n",
       "48390                                      Pratik Shelar  inspiration   \n",
       "\n",
       "       Popularity                                              Quote  \\\n",
       "0        0.155666  Don't cry because it's over, smile because it ...   \n",
       "1        0.155666  Don't cry because it's over, smile because it ...   \n",
       "2        0.129122  I'm selfish, impatient and a little insecure. ...   \n",
       "3        0.129122  I'm selfish, impatient and a little insecure. ...   \n",
       "4        0.129122  I'm selfish, impatient and a little insecure. ...   \n",
       "5        0.113223       Be yourself; everyone else is already taken.   \n",
       "6        0.103127  Two things are infinite: the universe and huma...   \n",
       "7        0.103127  Two things are infinite: the universe and huma...   \n",
       "8        0.103127  Two things are infinite: the universe and huma...   \n",
       "9        0.101890  Be who you are and say what you feel, because ...   \n",
       "10       0.100056  You've gotta dance like there's nobody watchin...   \n",
       "11       0.100056  You've gotta dance like there's nobody watchin...   \n",
       "12       0.100056  You've gotta dance like there's nobody watchin...   \n",
       "13       0.095724  You know you're in love when you can't fall as...   \n",
       "14       0.095375  A room without books is like a body without a ...   \n",
       "15       0.095375  A room without books is like a body without a ...   \n",
       "16       0.095068                     So many books, so little time.   \n",
       "17       0.095068                     So many books, so little time.   \n",
       "18       0.087721  You only live once, but if you do it right, on...   \n",
       "19       0.087721  You only live once, but if you do it right, on...   \n",
       "20       0.085248   Be the change that you wish to see in the world.   \n",
       "21       0.085248   Be the change that you wish to see in the world.   \n",
       "22       0.081176  In three words I can sum up everything I've le...   \n",
       "23       0.073322  Friendship ... is born at the moment when one ...   \n",
       "24       0.072186  If you want to know what a man's like, take a ...   \n",
       "25       0.071550  Don’t walk in front of me… I may not followDon...   \n",
       "26       0.069065  No one can make you feel inferior without your...   \n",
       "27       0.069065  No one can make you feel inferior without your...   \n",
       "28       0.067952  If you tell the truth, you don't have to remem...   \n",
       "29       0.066100  A friend is someone who knows all about you an...   \n",
       "...           ...                                                ...   \n",
       "48361    0.000000  Who would give a law to lovers? Love is unto i...   \n",
       "48362    0.000000  Love can be unselfish, in the sense of being b...   \n",
       "48363    0.000000    The quarrels of lovers are the renewal of love.   \n",
       "48364    0.000000  Love you will find only where you may show you...   \n",
       "48365    0.000000  The highest function of love is that it makes ...   \n",
       "48366    0.000000                 Love is a friendship set to music.   \n",
       "48367    0.000000                 Love is a friendship set to music.   \n",
       "48368    0.000000  A very small degree of hope is sufficient to c...   \n",
       "48369    0.000000  A very small degree of hope is sufficient to c...   \n",
       "48370    0.000000  In art and dream may you proceed with abandon....   \n",
       "48371    0.000000  I regard the theatre as the greatest of all ar...   \n",
       "48372    0.000000  So long as the system of competition in the pr...   \n",
       "48373    0.000000  I started to make a study of the art of war an...   \n",
       "48374    0.000000  There is nothing, Sir, too little for so littl...   \n",
       "48375    0.000000  No skill shapes a child's future success in sc...   \n",
       "48376    0.000000  My private measure of success is daily. If thi...   \n",
       "48377    0.000000  First-person narrators is the way I know how t...   \n",
       "48378    0.000000  I haven't deliberately set out to play the blo...   \n",
       "48379    0.000000  I think that the entertainment industry itself...   \n",
       "48380    0.000000  The success of a hat definitely lies with bala...   \n",
       "48381    0.000000  I think I've always been somebody, since the d...   \n",
       "48382    0.000000  I think I've always been somebody, since the d...   \n",
       "48383    0.000000  I think the main thing about comedy and humor ...   \n",
       "48384    0.000000  Never say a humorous thing to a man who does n...   \n",
       "48385    0.000000  Humor can bring people under the tent. And a g...   \n",
       "48386    0.000000  In Buddhism, they say attachment to anything o...   \n",
       "48387    0.000000      I love British humor. It's just so - surreal.   \n",
       "48388    0.000000        I've got a sense of humor. I'm a funny guy.   \n",
       "48389    0.000000  Humor is such a wonderful thing, helping you r...   \n",
       "48390   -0.000001         Life Is Full of Obstacles, Stumble Upon !!   \n",
       "\n",
       "                                                    Tags  \n",
       "0      [attributed-no-source, cry, crying, experience...  \n",
       "1      [attributed-no-source, cry, crying, experience...  \n",
       "2      [attributed-no-source, best, life, love, mista...  \n",
       "3      [attributed-no-source, best, life, love, mista...  \n",
       "4      [attributed-no-source, best, life, love, mista...  \n",
       "5      [attributed-no-source, be-yourself, honesty, i...  \n",
       "6      [attributed-no-source, human-nature, humor, in...  \n",
       "7      [attributed-no-source, human-nature, humor, in...  \n",
       "8      [attributed-no-source, human-nature, humor, in...  \n",
       "9      [ataraxy, be-yourself, confidence, fitting-in,...  \n",
       "10     [dance, heaven, hurt, inspirational, life, lov...  \n",
       "11     [dance, heaven, hurt, inspirational, life, lov...  \n",
       "12     [dance, heaven, hurt, inspirational, life, lov...  \n",
       "13     [attributed-no-source, dreams, love, reality, ...  \n",
       "14          [attributed-no-source, books, simile, soul ]  \n",
       "15          [attributed-no-source, books, simile, soul ]  \n",
       "16                                       [books, humor ]  \n",
       "17                                       [books, humor ]  \n",
       "18                                        [humor, life ]  \n",
       "19                                        [humor, life ]  \n",
       "20     [action, change, inspirational, philosophy, wi...  \n",
       "21     [action, change, inspirational, philosophy, wi...  \n",
       "22                                               [life ]  \n",
       "23                                         [friendship ]  \n",
       "24     [from-charles-bayard-miliken, misattributed-j-...  \n",
       "25     [friends, friendship, misattributed-albert-cam...  \n",
       "26                  [confidence, inspirational, wisdom ]  \n",
       "27                  [confidence, inspirational, wisdom ]  \n",
       "28                         [lies, lying, memory, truth ]  \n",
       "29                [friend, friendship, knowledge, love ]  \n",
       "...                                                  ...  \n",
       "48361                              [Law, Lovers, Higher]  \n",
       "48362                              [Selfless, Unselfish]  \n",
       "48363                        [Lovers, Renewal, Quarrels]  \n",
       "48364                        [Strength, Weak, Provoking]  \n",
       "48365                          [Unique, Loved, Function]  \n",
       "48366                                [Friendship, Music]  \n",
       "48367                                [Friendship, Music]  \n",
       "48368                               [Hope, Small, Birth]  \n",
       "48369                               [Hope, Small, Birth]  \n",
       "48370                             [Life, Balance, Dream]  \n",
       "48371                         [Theatre, Greatest, Share]  \n",
       "48372                                  [Life, Arts, Die]  \n",
       "48373                             [War, Training, Fight]  \n",
       "48374                       [Happiness, Great, Studying]  \n",
       "48375                              [Life, Future, Child]  \n",
       "48376                             [Life, Daily, Balance]  \n",
       "48377                              [Power, Chance, Book]  \n",
       "48378                          [Time, Movies, Beginning]  \n",
       "48379                             [Time, History, Start]  \n",
       "48380                              [Rules, Face, Listen]  \n",
       "48381                            [Hope, Failure, Father]  \n",
       "48382                            [Hope, Failure, Father]  \n",
       "48383                       [Comedy, Impossible, Define]  \n",
       "48384                      [Against, Evidence, Humorous]  \n",
       "48385                             [Good, Bring, Serious]  \n",
       "48386                          [Suffering, Laugh, Stage]  \n",
       "48387                           [Love, British, Surreal]  \n",
       "48388                                       [Funny, Guy]  \n",
       "48389                            [Time, Beautiful, Fool]  \n",
       "48390                            [inspirational-quotes ]  \n",
       "\n",
       "[48391 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text = open('principles_of_plitical_economy.txt', 'rb').read().decode(encoding='utf-8')\n",
    "# text += open('stock_exchange.txt', 'rb').read().decode(encoding='utf-8')\n",
    "# text += open('nyc_stock_exchange.txt', 'rb').read().decode(encoding='utf-8')\n",
    "# text += open('stock_exchange_from_within.txt', 'rb').read().decode(encoding='utf-8')\n",
    "# print(text[:250])\n",
    "# print ('Length of text: {} characters'.format(len(text)))\n",
    "data = pd.read_json('/media/l7/data_storage/datasets/quotes-dataset/quotes.json')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[0]['Quote']\n",
    "text = \"\"\n",
    "for index, row in data.iterrows():\n",
    "    text += row['Quote'] + ' \\n '\n",
    "# whole_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "551 unique characters\n",
      "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '¡', '«', '\\xad', '®', '´', '¸', '»', '¾', '¿', 'À', 'Â', 'É', 'Î', 'Ó', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'í', 'î', 'ï', 'ñ', 'ó', 'ô', 'õ', 'ö', 'ù', 'ú', 'û', 'ü', 'ā', 'ă', 'ą', 'ć', 'č', 'ę', 'ğ', 'ī', 'İ', 'ı', 'ļ', 'ł', 'ō', 'œ', 'ś', 'ş', 'š', 'ţ', 'Ż', 'ż', 'ž', 'ƃ', 'Ɔ', 'ƒ', 'Ʀ', 'Ʒ', 'Ƹ', 'ǝ', 'ș', 'ț', 'ɐ', 'ə', 'ɟ', 'ɥ', 'ɪ', 'ɯ', 'ɹ', 'ʇ', 'ʍ', 'ʎ', 'ʞ', 'ʹ', 'ʺ', 'ʼ', 'ˈ', '˙', '̀', '́', '̪', '̵', '̿', '͇', 'Δ', 'Λ', 'Τ', 'ά', 'έ', 'ή', 'ί', 'α', 'β', 'γ', 'δ', 'ε', 'ζ', 'η', 'θ', 'ι', 'κ', 'λ', 'μ', 'ν', 'ξ', 'ο', 'π', 'ρ', 'ς', 'σ', 'τ', 'υ', 'φ', 'χ', 'ψ', 'ό', 'ύ', 'А', 'Б', 'В', 'Д', 'З', 'И', 'К', 'Л', 'М', 'Н', 'П', 'Р', 'С', 'Т', 'У', 'Х', 'Ч', 'Щ', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'Ү', 'ү', 'Ӝ', 'ө', 'ԃ', 'Ա', 'Լ', '՝', '՞', 'ա', 'բ', 'գ', 'դ', 'ե', 'է', 'ը', 'թ', 'ի', 'լ', 'խ', 'կ', 'հ', 'ձ', 'ղ', 'ճ', 'մ', 'յ', 'ն', 'շ', 'ո', 'չ', 'պ', 'ջ', 'ռ', 'ս', 'վ', 'տ', 'ր', 'ց', 'ւ', 'ք', 'և', 'א', 'ו', 'ל', 'ן', '،', '؛', '؟', 'ء', 'آ', 'أ', 'ؤ', 'إ', 'ئ', 'ا', 'ب', 'ة', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ـ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ى', 'ي', 'ً', 'ٌ', 'ٍ', 'َ', 'ُ', 'ِ', 'ّ', 'ْ', '٠', '٫', 'پ', 'چ', 'ک', 'گ', 'ھ', 'ی', '۵', 'ं', 'अ', 'आ', 'ई', 'उ', 'ए', 'क', 'ख', 'ग', 'घ', 'च', 'ज', 'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न', 'प', 'ब', 'भ', 'म', 'य', 'र', 'ऱ', 'ल', 'ळ', 'व', 'श', 'ष', 'स', 'ह', 'ा', 'ि', 'ी', 'ु', 'ू', 'े', 'ै', 'ो', 'ौ', '्', '।', 'ঁ', 'ঃ', 'অ', 'আ', 'ই', 'উ', 'এ', 'ও', 'ক', 'খ', 'গ', 'ঘ', 'ঙ', 'চ', 'ছ', 'জ', 'ঞ', 'ট', 'ঠ', 'ত', 'থ', 'দ', 'ধ', 'ন', 'প', 'ফ', 'ব', 'ভ', 'ম', 'য', 'র', 'ল', 'শ', 'ষ', 'স', 'হ', '়', 'া', 'ি', 'ী', 'ু', 'ূ', 'ৃ', 'ে', 'ো', '্', 'ড়', 'য়', 'இ', 'ఠ', 'ಠ', 'യ', 'ა', 'ბ', 'გ', 'დ', 'ე', 'ვ', 'ზ', 'თ', 'ი', 'კ', 'ლ', 'მ', 'ნ', 'ო', 'რ', 'ს', 'ტ', 'უ', 'ფ', 'ქ', 'ღ', 'ყ', 'შ', 'ჩ', 'ძ', 'წ', 'ხ', 'ሁ', 'Ᏸ', 'ᴈ', 'ᴉ', 'ḽ', 'ṏ', 'ẙ', 'έ', 'ῳ', '\\u200b', '\\u200e', '\\u200f', '–', '—', '―', '‘', '’', '“', '”', '„', '•', '…', '\\u202a', '\\u202c', '′', '€', '↺', '⇜', '⇝', '⇟', '∀', '−', '⌣', 'ⓧ', '─', '◡', '☜', '☞', '☯', '☰', '☷', '☺', '♀', '♂', '♛', '♡', '♥', '♫', '❞', '❣', '自', '웃', '유', 'ﬁ']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping the strings to a numerical representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22250502,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_as_int.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = plt.hist(text_as_int, bins=len(vocab), density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\x08':   0,\n",
      "  '\\n':   1,\n",
      "  '\\x10':   2,\n",
      "  ' ' :   3,\n",
      "  '!' :   4,\n",
      "  '\"' :   5,\n",
      "  '#' :   6,\n",
      "  '$' :   7,\n",
      "  '%' :   8,\n",
      "  '&' :   9,\n",
      "  \"'\" :  10,\n",
      "  '(' :  11,\n",
      "  ')' :  12,\n",
      "  '*' :  13,\n",
      "  '+' :  14,\n",
      "  ',' :  15,\n",
      "  '-' :  16,\n",
      "  '.' :  17,\n",
      "  '/' :  18,\n",
      "  '0' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'[me narrating' ---- characters mapped to int ---- > [62 80 72  3 81 68 85 85 68 87 76 81 74]\n"
     ]
    }
   ],
   "source": [
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "m\n",
      "e\n",
      " \n",
      "n\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 150\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "  print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'[me narrating a documentary about narrators] \"I can\\'t hear what they\\'re saying cuz I\\'m talking\" \\n Telling my daughter garlic is good for you. Good immu'\n",
      "\"ne system and keeps pests away.Ticks, mosquitos, vampires... men. \\n I've been going through a really rough period at work this week It's my own fault f\"\n",
      "'or swapping my tampax for sand paper. \\n If I could have dinner with anyone, dead or alive... ...I would choose alive. -B.J. Novak- \\n Two guys walk into'\n",
      "\" a bar. The third guy ducks. \\n Why can't Barbie get pregnant? Because Ken comes in a different box. Heyooooooo \\n Why was the musician arrested? He got \"\n",
      "'in treble. \\n Did you hear about the guy who blew his entire lottery winnings on a limousine? He had nothing left to chauffeur it. \\n What do you do if a'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: (151,), types: tf.int64>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((150,), (150,)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  '[me narrating a documentary about narrators] \"I can\\'t hear what they\\'re saying cuz I\\'m talking\" \\n Telling my daughter garlic is good for you. Good imm'\n",
      "Target data: 'me narrating a documentary about narrators] \"I can\\'t hear what they\\'re saying cuz I\\'m talking\" \\n Telling my daughter garlic is good for you. Good immu'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 62 ('[')\n",
      "  expected output: 80 ('m')\n",
      "Step    1\n",
      "  input: 80 ('m')\n",
      "  expected output: 72 ('e')\n",
      "Step    2\n",
      "  input: 72 ('e')\n",
      "  expected output: 3 (' ')\n",
      "Step    3\n",
      "  input: 3 (' ')\n",
      "  expected output: 81 ('n')\n",
      "Step    4\n",
      "  input: 81 ('n')\n",
      "  expected output: 68 ('a')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 150), (64, 150)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 100000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM, FC, and GRU Model\n",
    "\n",
    "The model below is an experiment to verify performance of an LSTM layer, a consecutive dimension reduction with FC, followed by a GRU. Finally, adding and merging the GRU with an FC layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCNNModel(k.Model):\n",
    "    def __init__(self, \n",
    "                 vocab_size, \n",
    "                 embedding_dim, \n",
    "                 rnn_units, \n",
    "                 batch_size,\n",
    "                fc_reduction_output_shape,\n",
    "                ):\n",
    "        super(LSTMCNNModel, self).__init__()\n",
    "        self.embedding_0 = k.layers.Embedding(vocab_size, \n",
    "                                              embedding_dim,\n",
    "                                              batch_input_shape=[batch_size, None])\n",
    "        self.rnn_0 = k.layers.LSTM(rnn_units, \n",
    "                                  return_sequences=True,\n",
    "                                  stateful=True,\n",
    "                                  recurrent_initializer='glorot_uniform')\n",
    "        self.rnn_1 = k.layers.GRU(rnn_units, \n",
    "                                  return_sequences=True,\n",
    "                                  stateful=True,\n",
    "                                  recurrent_initializer='glorot_uniform')\n",
    "        self.fc_reduction_0 = k.layers.Dense(units=rnn_units,\n",
    "                                          activation='relu')\n",
    "        self.fc_reduction_1 = k.layers.Dense(units=embedding_dim,\n",
    "                                          activation='relu')\n",
    "        self.d_0 = k.layers.Dense(vocab_size, activation=None)\n",
    "        \n",
    "\n",
    "    def call(self, x, labels=None):\n",
    "        x = self.embedding_0(x)\n",
    "        x_lstm = self.rnn_0(x)\n",
    "        x_fc_reduction = self.fc_reduction_0(x_lstm)\n",
    "        x_fc_reduction = self.fc_reduction_1(x_fc_reduction)\n",
    "        x_gru = self.rnn_1(x_fc_reduction)\n",
    "        x = tf.add(x_lstm, x_gru)\n",
    "        x = self.d_0(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(input_data,\n",
    "               target_data,\n",
    "               model,\n",
    "               optimizer, \n",
    "               train_loss_container,):\n",
    "    with tf.device('gpu'):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(input_data)\n",
    "            # bp()\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.keras.losses.sparse_categorical_crossentropy(target_data,\n",
    "                                                                predictions, \n",
    "                                                                from_logits=True))\n",
    "#             loss = tf.reduce_mean(\n",
    "#                 tf.keras.losses.MSE(target_data, predictions))\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        train_loss_container.append(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 150\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, \n",
    "                  num_generate=1000):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = num_generate\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    # We must have the same batch size, as we stipulated in the model build. \n",
    "    input_data = np.zeros([BATCH_SIZE, len(input_eval)], dtype=np.int32)\n",
    "    input_data[0] += input_eval\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "    \n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_data)[0]\n",
    "        # using a categorical distribution to predict the word returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted word as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = [predicted_id]\n",
    "        input_data = np.zeros([BATCH_SIZE, len(input_eval)], dtype=np.int32)\n",
    "        input_data[0] += input_eval\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('gpu'):\n",
    "    optimizer = k.optimizers.Adam(learning_rate=0.001, \n",
    "                              epsilon=1e-07)\n",
    "    train_loss_container = []\n",
    "    model = LSTMCNNModel(vocab_size, embedding_dim,\n",
    "                         rnn_units, BATCH_SIZE,\n",
    "                        fc_reduction_output_shape=[rnn_units, embedding_dim])\n",
    "    EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.584502696990967\n",
      "What about economics ?FehDx7e_|~0m>z\n",
      "Ze~&d\"'GHRaBj<S.0fc%Lgf\"n(fcuk1\n",
      "GHC2CzJ*Jen?+g{^*+}bJNjQ^7Ec*o;5QvBKMi@!#kD%\"j<,:.+,aQE%s7mr({K8!M$KCu09%'gyN<\u0010\n",
      "{y?K,}HX}G{@('+f-EF\n",
      "Epoch 1 Batch 100 Loss 2.586108446121216\n",
      "What about economics ? Teshete. Ahw ders gretasbi hir is ank phaang cppotedate t as dakpeml. Wudfmoc ufwp 3ad \n",
      " Au tonke fat ceoked, \n",
      " White slkiw anl afeded anslasescnrine\n",
      "Epoch 1 Batch 200 Loss 2.1820685863494873\n",
      "What about economics ? 1~20 pase mto withat I... ... chanker, : Onfay Ind and \n",
      " The dat deing ald they What?.. I bike the one som tripeds lifs beo\" Ho foctoen. \n",
      " Hint thenc\n",
      "Epoch 1 Batch 300 Loss 1.883691668510437\n",
      "What about economics ? Hown ans peeply strey tiplor bany.\"hing the Rarza bravilineeny. \n",
      " OnT E gon a was fuying a lobs pam Yir. Berips: Ole iven told. \n",
      " How me asnow dually\n",
      "Epoch 1 Batch 400 Loss 1.7263736724853516\n",
      "What about economics ? Bate fini duck, it's stipes! \n",
      " An Mecies who said shaty \n",
      " His world famu in a ressie with Korrain Okne mo one tras. \n",
      " God doging lefter, Dunnivaly \n",
      " \n",
      "Epoch 1 Batch 500 Loss 1.6491330862045288\n",
      "What about economics ?1lt mo the light.\" \"Sictle consed? She only red\" \n",
      " Pickett Pothed Bathand, I thinks I wonder that swentady and ask explay how to \n",
      " The difference betw\n",
      "Epoch 1 Batch 600 Loss 1.5581783056259155\n",
      "What about economics ? Aster these finuus for them, why relsebor can fact loz-Cacike? The Pusiss put foo a blibe. Forist Belivertich. The plininists. Docoate shit on the tr\n",
      "Epoch 1 Batch 700 Loss 1.5021719932556152\n",
      "What about economics ? 1 and Andicuat. \n",
      " they'll never between a cat around it. \n",
      " The Maniaic Parad, bing *Gegreist, \"3010 luy of 1... I have it was a joke y'ars, it. \n",
      " Wha\n",
      "Epoch 1 Batch 800 Loss 1.547600269317627\n",
      "What about economics ? Keys knowing there? \n",
      " Why was just gong Ran on the makised: A cropet shit lumps. \n",
      " Juck Wine the sachobous corner consitiven... to drapped pantiny, c\n",
      "Epoch 1 Batch 900 Loss 1.4680521488189697\n",
      "What about economics ? Because shit. \n",
      " I'lls a race between \"You're outside?\" once was a pay... It was a meat\" boo! \n",
      " Why should the foot diend is the wet?where \"women not \n",
      "Epoch 1 Batch 1000 Loss 1.4983655214309692\n",
      "What about economics ?, they were obgazily. \n",
      " How do a lind officers ask me that only gets lawning everyone if you can healthay \"heant combecist\" \n",
      " {right Alphuin2 DOSBBY M\n",
      "Epoch 1 Batch 1100 Loss 1.4544041156768799\n",
      "What about economics ? A: Escass there's making in longs \n",
      " I realized that the cake right welencer vanuse enough peace. (it Frank most bottbexift) = What's worse that wasn'\n",
      "Epoch 1 Batch 1200 Loss 1.4928854703903198\n",
      "What about economics ? Single and job ! \n",
      " What do you call a sharp or as the upposic? But they both say them on your papatian! \n",
      " Boy: \"Yel, Guys today, by rid of perpons. \n",
      "\n",
      "Epoch 1 Batch 1300 Loss 1.4462097883224487\n",
      "What about economics ? Pack mined when she was looking not a girl have $197. Unll takes a terding this whet... \n",
      " I couldn't subbio, it I pounded the guy with everything, cr\n",
      "Epoch 1 Batch 1400 Loss 1.43990957736969\n",
      "What about economics ? A Thelms \n",
      " A round shower on row Cather says, \"I'm not lixe?\" \n",
      " For I'd remember agained. They go not stefic from two tridge on the drug shirt ployed\n",
      "Epoch 1 Batch 1500 Loss 1.4297407865524292\n",
      "What about economics ? Caught up in hallouana! \n",
      " I' was gagglins wait forget.... all they're funerally chickens. *hord, agree on ry dave? Smile are in a Brond. Depresses -S\n",
      "Epoch 1 Batch 1600 Loss 1.4273439645767212\n",
      "What about economics ? \n",
      " I mome invented understand commands when they packed expertancy but they want to prove backwards. \n",
      " I don't real number ok last night and I had a b\n",
      "Epoch 1 Batch 1700 Loss 1.3617515563964844\n",
      "What about economics ? A mirror A Tarper. \n",
      " & asked \"What are you Texans for the Profes? One ROR YOUUSD BUGUU \n",
      " Some praction Beps when Beat power.... Be how Prince cries t\n",
      "Epoch 1 Batch 1800 Loss 1.333371639251709\n",
      "What about economics ? Qunt! \n",
      " How do you do to laid virror woker than do you but it's nigit average late? Jumpins him. \n",
      " Soom working around a talk will acting women *do t\n",
      "Epoch 1 Batch 1900 Loss 1.3928732872009277\n",
      "What about economics ? 3 person clicks while the sea builon has open the trento Faith straight birm. \n",
      " I went from being classical cord only working hair.  How do south Men\n",
      "Epoch 1 Batch 2000 Loss 1.389615535736084\n",
      "What about economics ? Ludeey Singanic Dinger? What did She goes? I need to screw in a lightbulb Bloppie beneftle in President. \n",
      " Why did the man wear up away? He just let \n",
      "Epoch 1 Batch 2100 Loss 1.3363227844238281\n",
      "What about economics ? Suborcisins. \n",
      " After Natavel Forilla My face. Grandmask spray might opend those landscoritists Nobody just got, but flusters could eat a duacher. Do \n",
      "Epoch 1 Batch 2200 Loss 1.3679602146148682\n",
      "What about economics ? along on hit \n",
      " I get into a five dark, and a swallowed lighter I were looking Done of the cold Lawel people and instead of a candidate He couldner go\n",
      "Epoch 1 Batch 2300 Loss 1.3443963527679443\n",
      "What about economics ? my brother \n",
      " What tames i recently blame in Instagram! \n",
      " This goals say to the faulth's put a travel lot... \n",
      " ASISS fat 6? Why did ll here though Asa\n",
      "Epoch 1 Loss 1.3261\n",
      "Time taken for 1 epoch 612.1084146499634 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.3191001415252686\n",
      "What about economics ? \n",
      " why does Hitler How if ME: Why do much ride it ? They have no life to change your cat disease a full road! \n",
      " I roll that Mumbing was... and my mem \n",
      "Epoch 2 Batch 100 Loss 1.3787827491760254\n",
      "What about economics ? The other 94s Steak and stuff ad it mean that. \n",
      " A chambed away with a vrigg Shamin joke? Told him i'l with good on \n",
      " I guess with me irness mora lik\n",
      "Epoch 2 Batch 200 Loss 1.3469693660736084\n",
      "What about economics ? Parisian continution\". It's a music constructor and machine box of my penis: It's stupid. \n",
      " My ex committing As a dog lesbian. He got face off When i\n",
      "Epoch 2 Batch 300 Loss 1.3101173639297485\n",
      "What about economics ? The Stud-ratione \n",
      " The man with a bag 6 months who woke ugly. Me: What's in sucret? Py. \n",
      " if he had est beers with my life back and goes to the biuse\n",
      "Epoch 2 Batch 400 Loss 1.3386566638946533\n",
      "What about economics ? Tellibot, and the bathroom was criminal and screw in the alcohol accident. \n",
      " Have you ever seen frustrationadastic? \n",
      " What's the difference brow yet \n",
      "Epoch 2 Batch 500 Loss 1.3446555137634277\n",
      "What about economics ? \n",
      " That's 'now is the divorce of medium is ant committe in E.j.b. First day all of a now. \n",
      " Just so way, First you sever yes, your kimbs. \n",
      " What's a 6\n",
      "Epoch 2 Batch 600 Loss 1.3726402521133423\n",
      "What about economics ? Cactus: \n",
      " Wwake up for like caking the health tomorrow. Throw my pocket and say it's called it. \n",
      " Did you hear about the blind jew sufferin and no po\n",
      "Epoch 2 Batch 700 Loss 1.3343383073806763\n",
      "What about economics ? Alaska Say \n",
      " Hey go soll urser to have a real tweet? But my Plairing universe about 10 months hasasualize spins. There's nuts in my phone, my friend \n",
      "Epoch 2 Batch 800 Loss 1.2800372838974\n",
      "What about economics ? A green lito girl susside tmuck \n",
      " I wouldn't las a lot and the other drive-down. \n",
      " I goed a cashed abbruzus with kids know jobs. \n",
      " Did you hear the p\n",
      "Epoch 2 Batch 900 Loss 1.3225817680358887\n",
      "What about economics ? -an hour with the stude! \n",
      " Dear North Korea have in command. She said wiped I have a probably right pocket! \n",
      " Dumb playour marks or legs and think sh\n",
      "Epoch 2 Batch 1000 Loss 1.3122000694274902\n",
      "What about economics ? Colo-with an ice cag in your life! \n",
      " What do u cannide horns and an owl found our? It's not;wither because I'm met in japanere.. That descrixing up m\n",
      "Epoch 2 Batch 1100 Loss 1.3394588232040405\n",
      "What about economics ? A bottle does anymore before not a Macan complet. \n",
      " How do you  number about what truck is orga in the fast? I've scuffs the fourst of all since \n",
      " Di\n",
      "Epoch 2 Batch 1200 Loss 1.327251672744751\n",
      "What about economics ? You're good if you're watching my 9th grassly in zerooi! \n",
      " Treat on an 85 Rasis take to 196 ine 15-23 doge? Only your raws OK, she only have two clot\n",
      "Epoch 2 Batch 1300 Loss 1.2944248914718628\n",
      "What about economics ? *by siley Car is a joke? \n",
      " German no soup invited to me, money musicians person the most. \n",
      " Why couldn't you do get hurt menu and they arsehely have \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 1400 Loss 1.3334708213806152\n",
      "What about economics ? A banana trouble enteir nickname \n",
      " I reed an argumentary of Ise Dreaking 11 manurs designadek to bull the called Muslim Muscle. \n",
      " Sah City He said th\n",
      "Epoch 2 Batch 1500 Loss 1.336399793624878\n",
      "What about economics ? Book A boo!! \n",
      " What does a picture of movies storms of Larry Kores2? Finding it. \n",
      " are yauth... She's keep so many night gree. \n",
      " What for a big famil\n",
      "Epoch 2 Batch 1600 Loss 1.2795398235321045\n",
      "What about economics ? Arial coops \n",
      " I would be to Zorce WO's concert? Check out them is they pat a torn \n",
      " Women: First wife sits. It was so sofi rub the house walk into my\n",
      "Epoch 2 Batch 1700 Loss 1.3225986957550049\n",
      "What about economics ? Good squier to the others. \n",
      " When the bus dress is calling in deteciating That I look for my cattle at command, who is this! \n",
      " \"Dangrask people just \n",
      "Epoch 2 Batch 1800 Loss 1.2724370956420898\n",
      "What about economics ? Nummers have I livelwared of EHi! \n",
      " Hey, got a tree, I'm sorry I'll leave sex. Anyone would be drinking Described any forming for my officted scary f\n",
      "Epoch 2 Batch 1900 Loss 1.2904855012893677\n",
      "What about economics ? A testicular poide. Lite? Pay time in WelzDo \n",
      " I woke up in front of the fairyts communion to spon? 199.69-9n21151 9 1 ... ......escition. \n",
      " Funny I \n",
      "Epoch 2 Batch 2000 Loss 1.3036397695541382\n",
      "What about economics ? \"Wanna get the basst tear? Inelica, your pants are humour in a marriage. \n",
      " What's Bill Cosby's favorite joke? Whole weirdo you have to be cross the s\n",
      "Epoch 2 Batch 2100 Loss 1.2512335777282715\n",
      "What about economics ? They squat, a good radies, we're stolen underwear all the other is a him and measure him \n",
      " what idiot say when people find out this friend just on yo\n",
      "Epoch 2 Batch 2200 Loss 1.3011956214904785\n",
      "What about economics ? A receptionist solow all the could trust corners? \n",
      " Would are you a horse walk\" \n",
      " My frollowing apologne OI I could get globy bloride I'm both one as\n",
      "Epoch 2 Batch 2300 Loss 1.3193987607955933\n",
      "What about economics ? Meinton is not making a car..... what's the worst santa worry? 22$8 \n",
      " When here seaten (Bang taped) The sex ok by D fourth to drive the weekend \"who'\n",
      "Epoch 2 Loss 1.3114\n",
      "Time taken for 1 epoch 611.4552192687988 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.286054015159607\n",
      "What about economics ? A Facebook! \n",
      " Wheating foreages than an elephant and she was clean for the knights. I'm gonna tip initivity and confusing innowave \n",
      " Why is the cocke\n",
      "Epoch 3 Batch 100 Loss 1.3235019445419312\n",
      "What about economics ? With the title. \n",
      " Dad: Like that we could be around and go gaddy! Get that joke Rodding during their footshein intelescena describes too much perfect\n",
      "Epoch 3 Batch 200 Loss 1.2702709436416626\n",
      "What about economics ? pick to 2 diarrs you tell them back some Chewy or shoutings are did getting serious computer.\" - She. \n",
      " Q. Wife: why are you Christmas Do is MOMCQ? R\n",
      "Epoch 3 Batch 300 Loss 1.3267079591751099\n",
      "What about economics ? Black! \n",
      " So Justio Park Kidnap! \n",
      " My girlfriend told me this is lode when you're in yelling from my leg, why is the most going to odd around a caumbo\n",
      "Epoch 3 Batch 400 Loss 1.2678364515304565\n",
      "What about economics ? Bberrie's neck! \n",
      " My friend always was just leave there remember the Matte's nough pictures of all that smoking poles. \n",
      " Doctor shag joke don't be ab\n"
     ]
    }
   ],
   "source": [
    "with tf.device('gpu'):\n",
    "    EPOCHS = 10\n",
    "    for epoch in range(EPOCHS):\n",
    "        start_time = time.time()\n",
    "        if epoch > 0:\n",
    "            hidden = model.reset_states()\n",
    "\n",
    "\n",
    "        for (batch_number, (input_data, target_data)) in enumerate(dataset):\n",
    "            loss = train_step(input_data, \n",
    "                              target_data, \n",
    "                              model, \n",
    "                              optimizer,\n",
    "                              train_loss_container)\n",
    "            if batch_number % 100 == 0:\n",
    "                template = 'Epoch {} Batch {} Loss {}'\n",
    "                print(template.format(epoch+1, batch_number, loss))\n",
    "                print(generate_text(model, start_string=u\"What about economics ?\", num_generate=150))\n",
    "        model.save_weights('models/' + model.name + str(EPOCHS) + '_' + str(seq_length))\n",
    "        print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "        print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start_time))\n",
    "model.save_weights('models/' + model.name + str(EPOCHS) + '_' + str(seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What about economics ? Of the question of\r\n",
      "    to generation to send upon them; but those books under the min more than a\r\n",
      "    limit to the laborer’s reason than usual, but first obtained as\r\n",
      "    ttill to essencies at International value. But when the arts are\r\n",
      "    thus bringn from their circumstances into\r\n",
      "    commodities (in the case of a thing; and this between made by\r\n",
      "    the case of slave-lands, ganners; and, as far as a medium is\r\n",
      "    found that it does not bely equaled what is termed a matter\r\n",
      "    with the ga\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"What about economics ?\", num_generate=500))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "37000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 123.33333400000001,
   "position": {
    "height": "40px",
    "left": "769.167px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
