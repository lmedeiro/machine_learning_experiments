{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-rc1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)  # Ensure that we're using \"2.0.0-rc1\"\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as metrics\n",
    "import pdb\n",
    "from pdb import set_trace as bp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: \n",
    "This notebook builds on [this tensorflow tutorial](https://www.tensorflow.org/tutorials/text/text_generation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mmodels\u001b[0m/                             stock_exchange.txt\r\n",
      "nyc_stock_exchange.txt              text_response_experiment_0.ipynb\r\n",
      "principles_of_plitical_economy.txt  Untitled.ipynb\r\n",
      "stock_exchange_from_within.txt\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg EBook of Principles Of Political Economy by John\r\n",
      "Stuart Mill\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "This eBook is for the use of anyone anywhere at no cost and with almost no\r\n",
      "restrictions whatsoever. You may copy it, give it away or re-use it under\r\n",
      "the term\n",
      "Length of text: 2584696 characters\n"
     ]
    }
   ],
   "source": [
    "text = open('principles_of_plitical_economy.txt', 'rb').read().decode(encoding='utf-8')\n",
    "text += open('stock_exchange.txt', 'rb').read().decode(encoding='utf-8')\n",
    "text += open('nyc_stock_exchange.txt', 'rb').read().decode(encoding='utf-8')\n",
    "text += open('stock_exchange_from_within.txt', 'rb').read().decode(encoding='utf-8')\n",
    "print(text[:250])\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138 unique characters\n",
      "['\\n', '\\x0c', '\\r', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '\\xa0', '£', '§', '°', '¼', '½', '¾', 'É', '×', 'à', 'â', 'ä', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'î', 'ô', 'ö', 'ü', 'Œ', 'ε', 'η', 'ι', 'ρ', 'ς', 'σ', 'τ', 'φ', 'χ', 'ω', 'ά', 'ύ', 'ῆ', '–', '—', '‘', '’', '“', '”', '™', '⅛', '⅜', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping the strings to a numerical representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2584696,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_as_int.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  '\\x0c':   1,\n",
      "  '\\r':   2,\n",
      "  ' ' :   3,\n",
      "  '!' :   4,\n",
      "  '\"' :   5,\n",
      "  '#' :   6,\n",
      "  '$' :   7,\n",
      "  '%' :   8,\n",
      "  '&' :   9,\n",
      "  \"'\" :  10,\n",
      "  '(' :  11,\n",
      "  ')' :  12,\n",
      "  '*' :  13,\n",
      "  '+' :  14,\n",
      "  ',' :  15,\n",
      "  '-' :  16,\n",
      "  '.' :  17,\n",
      "  '/' :  18,\n",
      "  '0' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The Project G' ---- characters mapped to int ---- > [55 72 69  3 51 82 79 74 69 67 84  3 42]\n"
     ]
    }
   ],
   "source": [
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T\n",
      "h\n",
      "e\n",
      " \n",
      "P\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 256\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "  print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The Project Gutenberg EBook of Principles Of Political Economy by John\\r\\nStuart Mill\\r\\n\\r\\n\\r\\n\\r\\nThis eBook is for the use of anyone anywhere at no cost and with almost no\\r\\nrestrictions whatsoever. You may copy it, give it away or re-use it under\\r\\nthe terms of th'\n",
      "'e Project Gutenberg License included with this eBook or\\r\\nonline at http://www.gutenberg.org/license\\r\\n\\r\\n\\r\\n\\r\\nTitle: Principles Of Political Economy\\r\\n\\r\\nAuthor: John Stuart Mill\\r\\n\\r\\nRelease Date: September 27, 2009 [Ebook #30107]\\r\\n\\r\\nLanguage: English\\r\\n\\r\\nCharacte'\n",
      "'r set encoding: UTF-8\\r\\n\\r\\n\\r\\n***START OF THE PROJECT GUTENBERG EBOOK PRINCIPLES OF POLITICAL ECONOMY***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                     Principles Of Political Economy\\r\\n\\r\\n                                    By\\r\\n\\r\\n                             John Stuart Mill\\r'\n",
      "'\\n\\r\\n                Abridged, with Critical, Bibliographical,\\r\\n\\r\\n                   and Explanatory Notes, and a Sketch\\r\\n\\r\\n                   of the History of Political Economy,\\r\\n\\r\\n                                    By\\r\\n\\r\\n                       J. Laurence'\n",
      "' Laughlin, Ph. D.\\r\\n\\r\\n      Assistant Professor of Political Economy in Harvard University\\r\\n\\r\\n                        A Text-Book For Colleges.\\r\\n\\r\\n                                New York:\\r\\n\\r\\n                         D. Appleton And Company,\\r\\n\\r\\n             '\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: (257,), types: tf.int64>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <function split_input_target at 0x7f7f16e06840> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((256,), (256,)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'The Project Gutenberg EBook of Principles Of Political Economy by John\\r\\nStuart Mill\\r\\n\\r\\n\\r\\n\\r\\nThis eBook is for the use of anyone anywhere at no cost and with almost no\\r\\nrestrictions whatsoever. You may copy it, give it away or re-use it under\\r\\nthe terms of t'\n",
      "Target data: 'he Project Gutenberg EBook of Principles Of Political Economy by John\\r\\nStuart Mill\\r\\n\\r\\n\\r\\n\\r\\nThis eBook is for the use of anyone anywhere at no cost and with almost no\\r\\nrestrictions whatsoever. You may copy it, give it away or re-use it under\\r\\nthe terms of th'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 55 ('T')\n",
      "  expected output: 72 ('h')\n",
      "Step    1\n",
      "  input: 72 ('h')\n",
      "  expected output: 69 ('e')\n",
      "Step    2\n",
      "  input: 69 ('e')\n",
      "  expected output: 3 (' ')\n",
      "Step    3\n",
      "  input: 3 (' ')\n",
      "  expected output: 51 ('P')\n",
      "Step    4\n",
      "  input: 51 ('P')\n",
      "  expected output: 82 ('r')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 256), (64, 256)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 100000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicLSTMModel(k.Model):\n",
    "    def __init__(self, \n",
    "                 vocab_size, \n",
    "                 embedding_dim, \n",
    "                 rnn_units, \n",
    "                 batch_size):\n",
    "        super(BasicLSTMModel, self).__init__()\n",
    "        self.embedding_0 = k.layers.Embedding(vocab_size, \n",
    "                                              embedding_dim,\n",
    "                                              batch_input_shape=[batch_size, None])\n",
    "        self.rnn_0 = k.layers.LSTM(rnn_units, \n",
    "                                  return_sequences=True,\n",
    "                                  stateful=True,\n",
    "                                  recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "        self.d_0 = k.layers.Dense(vocab_size, activation=None)\n",
    "        \n",
    "\n",
    "    def call(self, x, labels=None):\n",
    "        # bp()\n",
    "        x = self.embedding_0(x)\n",
    "        x = self.rnn_0(x)\n",
    "        x = self.d_0(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(input_data,\n",
    "               target_data,\n",
    "               model,\n",
    "               optimizer, \n",
    "               train_loss_container,):\n",
    "    with tf.device('gpu'):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(input_data)\n",
    "            # bp()\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.keras.losses.sparse_categorical_crossentropy(target_data,\n",
    "                                                                predictions, \n",
    "                                                                from_logits=True))\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        train_loss_container.append(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, \n",
    "                  num_generate=1000):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = num_generate\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    # We must have the same batch size, as we stipulated in the model build. \n",
    "    input_data = np.zeros([BATCH_SIZE, len(input_eval)], dtype=np.int32)\n",
    "    input_data[0] += input_eval\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "    \n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_data)[0]\n",
    "        # using a categorical distribution to predict the word returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted word as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = [predicted_id]\n",
    "        input_data = np.zeros([BATCH_SIZE, len(input_eval)], dtype=np.int32)\n",
    "        input_data[0] += input_eval\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('gpu'):\n",
    "    optimizer = k.optimizers.Adam(learning_rate=0.001, \n",
    "                              epsilon=1e-07)\n",
    "    train_loss_container = []\n",
    "    model = BasicLSTMModel(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)\n",
    "    EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.927671432495117\n",
      "Epoch 1 Batch 100 Loss 2.3362631797790527\n",
      "Epoch 1 Loss 2.0144\n",
      "Time taken for 1 epoch 37.29335641860962 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.2708115577697754\n",
      "Epoch 2 Batch 100 Loss 1.6874340772628784\n",
      "Epoch 2 Loss 1.5850\n",
      "Time taken for 1 epoch 36.80041551589966 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.5684728622436523\n",
      "Epoch 3 Batch 100 Loss 1.4471094608306885\n",
      "Epoch 3 Loss 1.4592\n",
      "Time taken for 1 epoch 36.5369074344635 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.3949759006500244\n",
      "Epoch 4 Batch 100 Loss 1.31704843044281\n",
      "Epoch 4 Loss 1.2938\n",
      "Time taken for 1 epoch 37.40993547439575 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.2982511520385742\n",
      "Epoch 5 Batch 100 Loss 1.2892155647277832\n",
      "Epoch 5 Loss 1.2470\n",
      "Time taken for 1 epoch 36.66467475891113 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.device('gpu'):\n",
    "    # EPOCHS = 10\n",
    "    for epoch in range(EPOCHS):\n",
    "        start_time = time.time()\n",
    "        if epoch > 0:\n",
    "            hidden = model.reset_states()\n",
    "\n",
    "\n",
    "        for (batch_number, (input_data, target_data)) in enumerate(dataset):\n",
    "            loss = train_step(input_data, \n",
    "                              target_data, \n",
    "                              model, \n",
    "                              optimizer,\n",
    "                              train_loss_container)\n",
    "            if batch_number % 100 == 0:\n",
    "                template = 'Epoch {} Batch {} Loss {}'\n",
    "                print(template.format(epoch+1, batch_number, loss))\n",
    "        model.save_weights('models/' + model.name + str(EPOCHS) + '_' + str(seq_length))\n",
    "        print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "        print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start_time))\n",
    "model.save_weights('models/' + model.name + str(EPOCHS) + '_' + str(seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test hourses and botht-sellers.\r\n",
      "      § 1. Critics is important, onimon advants results in the Mrjan\r\n",
      "apigities, no reason, in 1850 causes the dail\r\n",
      "rach\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"test\", num_generate=150))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 123.33333400000001,
   "position": {
    "height": "40px",
    "left": "769.167px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
