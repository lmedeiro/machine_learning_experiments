{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-rc1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)  # Ensure that we're using \"2.0.0-rc1\"\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as k\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as metrics\n",
    "import pdb\n",
    "from pdb import set_trace as bp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: \n",
    "This notebook builds on [this tensorflow tutorial](https://www.tensorflow.org/tutorials/text/text_generation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mmodels\u001b[0m/                             stock_exchange.txt\r\n",
      "nyc_stock_exchange.txt              text_response_experiment_0.ipynb\r\n",
      "principles_of_plitical_economy.txt  text_response_experiment_RNN_CNN.ipynb\r\n",
      "stock_exchange_from_within.txt      VAE_text.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg EBook of Principles Of Political Economy by John\r\n",
      "Stuart Mill\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "This eBook is for the use of anyone anywhere at no cost and with almost no\r\n",
      "restrictions whatsoever. You may copy it, give it away or re-use it under\r\n",
      "the term\n",
      "Length of text: 2584696 characters\n"
     ]
    }
   ],
   "source": [
    "text = open('principles_of_plitical_economy.txt', 'rb').read().decode(encoding='utf-8')\n",
    "text += open('stock_exchange.txt', 'rb').read().decode(encoding='utf-8')\n",
    "text += open('nyc_stock_exchange.txt', 'rb').read().decode(encoding='utf-8')\n",
    "text += open('stock_exchange_from_within.txt', 'rb').read().decode(encoding='utf-8')\n",
    "print(text[:250])\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138 unique characters\n",
      "['\\n', '\\x0c', '\\r', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '\\xa0', '£', '§', '°', '¼', '½', '¾', 'É', '×', 'à', 'â', 'ä', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'î', 'ô', 'ö', 'ü', 'Œ', 'ε', 'η', 'ι', 'ρ', 'ς', 'σ', 'τ', 'φ', 'χ', 'ω', 'ά', 'ύ', 'ῆ', '–', '—', '‘', '’', '“', '”', '™', '⅛', '⅜', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping the strings to a numerical representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2584696,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_as_int.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = plt.hist(text_as_int, bins=len(vocab), density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  '\\x0c':   1,\n",
      "  '\\r':   2,\n",
      "  ' ' :   3,\n",
      "  '!' :   4,\n",
      "  '\"' :   5,\n",
      "  '#' :   6,\n",
      "  '$' :   7,\n",
      "  '%' :   8,\n",
      "  '&' :   9,\n",
      "  \"'\" :  10,\n",
      "  '(' :  11,\n",
      "  ')' :  12,\n",
      "  '*' :  13,\n",
      "  '+' :  14,\n",
      "  ',' :  15,\n",
      "  '-' :  16,\n",
      "  '.' :  17,\n",
      "  '/' :  18,\n",
      "  '0' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The Project G' ---- characters mapped to int ---- > [55 72 69  3 51 82 79 74 69 67 84  3 42]\n"
     ]
    }
   ],
   "source": [
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T\n",
      "h\n",
      "e\n",
      " \n",
      "P\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 256\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "  print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The Project Gutenberg EBook of Principles Of Political Economy by John\\r\\nStuart Mill\\r\\n\\r\\n\\r\\n\\r\\nThis eBook is for the use of anyone anywhere at no cost and with almost no\\r\\nrestrictions whatsoever. You may copy it, give it away or re-use it under\\r\\nthe terms of th'\n",
      "'e Project Gutenberg License included with this eBook or\\r\\nonline at http://www.gutenberg.org/license\\r\\n\\r\\n\\r\\n\\r\\nTitle: Principles Of Political Economy\\r\\n\\r\\nAuthor: John Stuart Mill\\r\\n\\r\\nRelease Date: September 27, 2009 [Ebook #30107]\\r\\n\\r\\nLanguage: English\\r\\n\\r\\nCharacte'\n",
      "'r set encoding: UTF-8\\r\\n\\r\\n\\r\\n***START OF THE PROJECT GUTENBERG EBOOK PRINCIPLES OF POLITICAL ECONOMY***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                     Principles Of Political Economy\\r\\n\\r\\n                                    By\\r\\n\\r\\n                             John Stuart Mill\\r'\n",
      "'\\n\\r\\n                Abridged, with Critical, Bibliographical,\\r\\n\\r\\n                   and Explanatory Notes, and a Sketch\\r\\n\\r\\n                   of the History of Political Economy,\\r\\n\\r\\n                                    By\\r\\n\\r\\n                       J. Laurence'\n",
      "' Laughlin, Ph. D.\\r\\n\\r\\n      Assistant Professor of Political Economy in Harvard University\\r\\n\\r\\n                        A Text-Book For Colleges.\\r\\n\\r\\n                                New York:\\r\\n\\r\\n                         D. Appleton And Company,\\r\\n\\r\\n             '\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: (257,), types: tf.int64>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((256,), (256,)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'The Project Gutenberg EBook of Principles Of Political Economy by John\\r\\nStuart Mill\\r\\n\\r\\n\\r\\n\\r\\nThis eBook is for the use of anyone anywhere at no cost and with almost no\\r\\nrestrictions whatsoever. You may copy it, give it away or re-use it under\\r\\nthe terms of t'\n",
      "Target data: 'he Project Gutenberg EBook of Principles Of Political Economy by John\\r\\nStuart Mill\\r\\n\\r\\n\\r\\n\\r\\nThis eBook is for the use of anyone anywhere at no cost and with almost no\\r\\nrestrictions whatsoever. You may copy it, give it away or re-use it under\\r\\nthe terms of th'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 55 ('T')\n",
      "  expected output: 72 ('h')\n",
      "Step    1\n",
      "  input: 72 ('h')\n",
      "  expected output: 69 ('e')\n",
      "Step    2\n",
      "  input: 69 ('e')\n",
      "  expected output: 3 (' ')\n",
      "Step    3\n",
      "  input: 3 (' ')\n",
      "  expected output: 51 ('P')\n",
      "Step    4\n",
      "  input: 51 ('P')\n",
      "  expected output: 82 ('r')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 256), (64, 256)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 100000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM and CNN Model\n",
    "\n",
    "The model below is an experiment to verify performance of an LSTM layer, which tries to learn sequential patterns, and a CNN layer, that is trying to learn spatial patterns from covariance matrix of the respective samples computed. The idea is then to combine this information, to attempt to produce a better predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCNNModel(k.Model):\n",
    "    def __init__(self, \n",
    "                 vocab_size, \n",
    "                 embedding_dim, \n",
    "                 rnn_units, \n",
    "                 batch_size):\n",
    "        super(LSTMCNNModel, self).__init__()\n",
    "        self.embedding_0 = k.layers.Embedding(vocab_size, \n",
    "                                              embedding_dim,\n",
    "                                              batch_input_shape=[batch_size, None])\n",
    "        self.rnn_0 = k.layers.LSTM(rnn_units, \n",
    "                                  return_sequences=True,\n",
    "                                  stateful=True,\n",
    "                                  recurrent_initializer='glorot_uniform')\n",
    "        self.cnn_0 = k.layers.Conv2D(filters=1, kernel_size=3,\n",
    "                                    activation='relu', data_format='channels_last')\n",
    "        self.max_0 = k.layers.MaxPool2D()\n",
    "        self.flatten = k.layers.Flatten()\n",
    "        \n",
    "        self.d_0 = k.layers.Dense(vocab_size, activation=None)\n",
    "        \n",
    "\n",
    "    def call(self, x, labels=None):\n",
    "        bp()\n",
    "        x = self.embedding_0(x)\n",
    "        x_lstm = self.rnn_0(x)\n",
    "        x_cov = tf.reduce_mean(x_lstm, axis=1)\n",
    "        x_cov_container = tf.zeros([64, 1024, 1024])\n",
    "        for i in range(x_cov.shape[0]):\n",
    "            x_cov_container[i, :, :] = tf.matmul(x_cov[i, :], x_cov[i, :], \n",
    "                                                transpose_b=True) / tf.reduce_sum(x_cov[i, :])\n",
    "        x_cov_container = tf.reshape(x_cov_container, np.append(x_cov_container.shape, 1))\n",
    "        x_cnn = self.cnn_0(x_cov_container)\n",
    "        x_cnn = self.max_0(x_cnn)\n",
    "        x_cnn = self.flatten(x_cnn)\n",
    "        x = tf.concat((x_lstm, x_cnn), axis=0)\n",
    "        x = self.d_0(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(input_data,\n",
    "               target_data,\n",
    "               model,\n",
    "               optimizer, \n",
    "               train_loss_container,):\n",
    "    with tf.device('gpu'):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(input_data)\n",
    "            # bp()\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.keras.losses.sparse_categorical_crossentropy(target_data,\n",
    "                                                                predictions, \n",
    "                                                                from_logits=True))\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        train_loss_container.append(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, \n",
    "                  num_generate=1000):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = num_generate\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    # We must have the same batch size, as we stipulated in the model build. \n",
    "    input_data = np.zeros([BATCH_SIZE, len(input_eval)], dtype=np.int32)\n",
    "    input_data[0] += input_eval\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "    \n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_data)[0]\n",
    "        # using a categorical distribution to predict the word returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted word as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = [predicted_id]\n",
    "        input_data = np.zeros([BATCH_SIZE, len(input_eval)], dtype=np.int32)\n",
    "        input_data[0] += input_eval\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('gpu'):\n",
    "    optimizer = k.optimizers.Adam(learning_rate=0.001, \n",
    "                              epsilon=1e-07)\n",
    "    train_loss_container = []\n",
    "    model = LSTMCNNModel(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)\n",
    "    EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-21-fd7f65364055>(25)call()\n",
      "-> x = self.embedding_0(x)\n",
      "(Pdb) n\n",
      "> <ipython-input-21-fd7f65364055>(26)call()\n",
      "-> x_lstm = self.rnn_0(x)\n",
      "(Pdb) n\n",
      "> <ipython-input-21-fd7f65364055>(27)call()\n",
      "-> x_cov = tf.reduce_mean(x_lstm, axis=1)\n",
      "(Pdb) n\n",
      "> <ipython-input-21-fd7f65364055>(28)call()\n",
      "-> x_cov_container = tf.zeros([64, 1024, 1024])\n",
      "(Pdb) n\n",
      "> <ipython-input-21-fd7f65364055>(29)call()\n",
      "-> for i in range(x_cov.shape[0]):\n",
      "(Pdb) n\n",
      "> <ipython-input-21-fd7f65364055>(30)call()\n",
      "-> x_cov_container[i, :, :] = tfp.stats.covariance(tf.reshape(x_cov[i,:],\n",
      "(Pdb) n\n",
      "> <ipython-input-21-fd7f65364055>(31)call()\n",
      "-> [x_cov[i,:].shape[0],\n",
      "(Pdb) n\n",
      "> <ipython-input-21-fd7f65364055>(32)call()\n",
      "-> 1]),\n",
      "(Pdb) n\n",
      "> <ipython-input-21-fd7f65364055>(33)call()\n",
      "-> sample_axis=0)\n",
      "(Pdb) n\n",
      "TypeError: 'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment\n",
      "> <ipython-input-21-fd7f65364055>(33)call()\n",
      "-> sample_axis=0)\n",
      "(Pdb) q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-c81ee7226bd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                               \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                               \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                               train_loss_container)\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_number\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mtemplate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Epoch {} Batch {} Loss {}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-73fc9abe21c0>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(input_data, target_data, model, optimizer, train_loss_container)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0;31m# bp()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             loss = tf.reduce_mean(\n",
      "\u001b[0;32m~/dev/tools/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-fd7f65364055>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, labels)\u001b[0m\n\u001b[1;32m     31\u001b[0m                                                                        [x_cov[i,:].shape[0], \n\u001b[1;32m     32\u001b[0m                                                                         1]),\n\u001b[0;32m---> 33\u001b[0;31m                                                             sample_axis=0)\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mx_cov_container\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cov_container\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cov_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mx_cnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cov_container\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exception'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c_call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_exception\u001b[0;34m(self, frame, arg)\u001b[0m\n\u001b[1;32m    111\u001b[0m                     and arg[0] is StopIteration and arg[2] is None):\n\u001b[1;32m    112\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;31m# Stop at the StopIteration or GeneratorExit exception when the user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# has set stopframe in a generator by issuing a return command, or a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.device('gpu'):\n",
    "    # EPOCHS = 10\n",
    "    for epoch in range(EPOCHS):\n",
    "        start_time = time.time()\n",
    "        if epoch > 0:\n",
    "            hidden = model.reset_states()\n",
    "\n",
    "\n",
    "        for (batch_number, (input_data, target_data)) in enumerate(dataset):\n",
    "            loss = train_step(input_data, \n",
    "                              target_data, \n",
    "                              model, \n",
    "                              optimizer,\n",
    "                              train_loss_container)\n",
    "            if batch_number % 100 == 0:\n",
    "                template = 'Epoch {} Batch {} Loss {}'\n",
    "                print(template.format(epoch+1, batch_number, loss))\n",
    "        model.save_weights('models/' + model.name + str(EPOCHS) + '_' + str(seq_length))\n",
    "        print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "        print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start_time))\n",
    "model.save_weights('models/' + model.name + str(EPOCHS) + '_' + str(seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(model, start_string=u\"test\", num_generate=150))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 123.33333400000001,
   "position": {
    "height": "40px",
    "left": "769.167px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
