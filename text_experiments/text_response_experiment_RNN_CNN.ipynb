{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-rc1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)  # Ensure that we're using \"2.0.0-rc1\"\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as k\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as metrics\n",
    "import pdb\n",
    "from pdb import set_trace as bp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: \n",
    "This notebook builds on [this tensorflow tutorial](https://www.tensorflow.org/tutorials/text/text_generation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mmodels\u001b[0m/                             stock_exchange.txt\r\n",
      "nyc_stock_exchange.txt              text_response_experiment_0.ipynb\r\n",
      "principles_of_plitical_economy.txt  text_response_experiment_RNN_CNN.ipynb\r\n",
      "stock_exchange_from_within.txt\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg EBook of Principles Of Political Economy by John\r\n",
      "Stuart Mill\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "This eBook is for the use of anyone anywhere at no cost and with almost no\r\n",
      "restrictions whatsoever. You may copy it, give it away or re-use it under\r\n",
      "the term\n",
      "Length of text: 1504941 characters\n"
     ]
    }
   ],
   "source": [
    "text = open('principles_of_plitical_economy.txt', 'rb').read().decode(encoding='utf-8')\n",
    "# text += open('stock_exchange.txt', 'rb').read().decode(encoding='utf-8')\n",
    "# text += open('nyc_stock_exchange.txt', 'rb').read().decode(encoding='utf-8')\n",
    "# text += open('stock_exchange_from_within.txt', 'rb').read().decode(encoding='utf-8')\n",
    "print(text[:250])\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 unique characters\n",
      "['\\n', '\\x0c', '\\r', ' ', '!', '\"', '#', '$', '%', '&', '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '£', '§', '¼', '½', '¾', 'É', '×', 'à', 'â', 'ä', 'æ', 'ç', 'è', 'é', 'ê', 'î', 'ô', 'ö', 'ü', 'Œ', 'ε', 'η', 'ι', 'ρ', 'ς', 'σ', 'τ', 'φ', 'χ', 'ω', 'ά', 'ύ', 'ῆ', '—', '‘', '’', '“', '”', '™']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping the strings to a numerical representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1504941,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_as_int.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = plt.hist(text_as_int, bins=len(vocab), density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  '\\x0c':   1,\n",
      "  '\\r':   2,\n",
      "  ' ' :   3,\n",
      "  '!' :   4,\n",
      "  '\"' :   5,\n",
      "  '#' :   6,\n",
      "  '$' :   7,\n",
      "  '%' :   8,\n",
      "  '&' :   9,\n",
      "  '(' :  10,\n",
      "  ')' :  11,\n",
      "  '*' :  12,\n",
      "  '+' :  13,\n",
      "  ',' :  14,\n",
      "  '-' :  15,\n",
      "  '.' :  16,\n",
      "  '/' :  17,\n",
      "  '0' :  18,\n",
      "  '1' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The Project G' ---- characters mapped to int ---- > [54 71 68  3 50 81 78 73 68 66 83  3 41]\n"
     ]
    }
   ],
   "source": [
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T\n",
      "h\n",
      "e\n",
      " \n",
      "P\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 150\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "  print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The Project Gutenberg EBook of Principles Of Political Economy by John\\r\\nStuart Mill\\r\\n\\r\\n\\r\\n\\r\\nThis eBook is for the use of anyone anywhere at no cost and '\n",
      "'with almost no\\r\\nrestrictions whatsoever. You may copy it, give it away or re-use it under\\r\\nthe terms of the Project Gutenberg License included with thi'\n",
      "'s eBook or\\r\\nonline at http://www.gutenberg.org/license\\r\\n\\r\\n\\r\\n\\r\\nTitle: Principles Of Political Economy\\r\\n\\r\\nAuthor: John Stuart Mill\\r\\n\\r\\nRelease Date: Septe'\n",
      "'mber 27, 2009 [Ebook #30107]\\r\\n\\r\\nLanguage: English\\r\\n\\r\\nCharacter set encoding: UTF-8\\r\\n\\r\\n\\r\\n***START OF THE PROJECT GUTENBERG EBOOK PRINCIPLES OF POLITICAL'\n",
      "' ECONOMY***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                     Principles Of Political Economy\\r\\n\\r\\n                                    By\\r\\n\\r\\n                             J'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: (151,), types: tf.int64>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((150,), (150,)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'The Project Gutenberg EBook of Principles Of Political Economy by John\\r\\nStuart Mill\\r\\n\\r\\n\\r\\n\\r\\nThis eBook is for the use of anyone anywhere at no cost and'\n",
      "Target data: 'he Project Gutenberg EBook of Principles Of Political Economy by John\\r\\nStuart Mill\\r\\n\\r\\n\\r\\n\\r\\nThis eBook is for the use of anyone anywhere at no cost and '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 54 ('T')\n",
      "  expected output: 71 ('h')\n",
      "Step    1\n",
      "  input: 71 ('h')\n",
      "  expected output: 68 ('e')\n",
      "Step    2\n",
      "  input: 68 ('e')\n",
      "  expected output: 3 (' ')\n",
      "Step    3\n",
      "  input: 3 (' ')\n",
      "  expected output: 50 ('P')\n",
      "Step    4\n",
      "  input: 50 ('P')\n",
      "  expected output: 81 ('r')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 150), (64, 150)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 100000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM and CNN Model\n",
    "\n",
    "The model below is an experiment to verify performance of an LSTM layer, which tries to learn sequential patterns, and a CNN layer, that is trying to learn spatial patterns from covariance matrix of the respective samples computed. The idea is then to combine this information, to attempt to produce a better predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCNNModel(k.Model):\n",
    "    def __init__(self, \n",
    "                 vocab_size, \n",
    "                 embedding_dim, \n",
    "                 rnn_units, \n",
    "                 batch_size,\n",
    "                fc_reduction_output_shape=[28, 28],\n",
    "                ):\n",
    "        super(LSTMCNNModel, self).__init__()\n",
    "        self.embedding_0 = k.layers.Embedding(vocab_size, \n",
    "                                              embedding_dim,\n",
    "                                              batch_input_shape=[batch_size, None])\n",
    "        self.rnn_0 = k.layers.LSTM(rnn_units, \n",
    "                                  return_sequences=True,\n",
    "                                  stateful=True,\n",
    "                                  recurrent_initializer='glorot_uniform')\n",
    "        self.fc_reduction = k.layers.Dense(units=np.multiply(*fc_reduction_output_shape),\n",
    "                                          activation='relu')\n",
    "        self.fc_reduction_output_shape = fc_reduction_output_shape\n",
    "        self.cnn_0 = k.layers.Conv2D(filters=1, kernel_size=3,\n",
    "                                    activation='relu', data_format='channels_last')\n",
    "        self.max_0 = k.layers.MaxPool2D()\n",
    "        self.flatten = k.layers.Flatten()\n",
    "        self.d_0 = k.layers.Dense(vocab_size, activation=None)\n",
    "        \n",
    "\n",
    "    def call(self, x, labels=None):\n",
    "        # bp()\n",
    "        x = self.embedding_0(x)\n",
    "        x_lstm = self.rnn_0(x)\n",
    "        x_lstm = self.flatten(x_lstm)\n",
    "        x_fc_reduction = self.fc_reduction(x_lstm)\n",
    "        x_fc_reduction = tf.reshape(x_fc_reduction, \n",
    "                                    shape=[x_fc_reduction.shape[0],\n",
    "                                          self.fc_reduction_output_shape[0],\n",
    "                                          self.fc_reduction_output_shape[1],\n",
    "                                          1])\n",
    "        x_cnn = self.cnn_0(x_fc_reduction)\n",
    "        x_cnn = self.max_0(x_cnn)\n",
    "        x_cnn = self.flatten(x_cnn)\n",
    "        x = tf.concat((x_lstm, x_cnn), axis=1)\n",
    "        x = self.d_0(x)\n",
    "        # bp()\n",
    "        return x\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(input_data,\n",
    "               target_data,\n",
    "               model,\n",
    "               optimizer, \n",
    "               train_loss_container,):\n",
    "    with tf.device('gpu'):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(input_data)\n",
    "            # bp()\n",
    "            # loss = tf.reduce_mean(\n",
    "            #     tf.keras.losses.sparse_categorical_crossentropy(target_data,\n",
    "            #                                                     predictions, \n",
    "            #                                                     from_logits=False))\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.keras.losses.MSE(target_data, predictions))\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        train_loss_container.append(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 150\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, \n",
    "                  num_generate=1000):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = num_generate\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    # We must have the same batch size, as we stipulated in the model build. \n",
    "    input_data = np.zeros([BATCH_SIZE, len(input_eval)], dtype=np.int32)\n",
    "    input_data[0] += input_eval\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "    \n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_data)[0]\n",
    "        # using a categorical distribution to predict the word returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted word as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = [predicted_id]\n",
    "        input_data = np.zeros([BATCH_SIZE, len(input_eval)], dtype=np.int32)\n",
    "        input_data[0] += input_eval\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('gpu'):\n",
    "    optimizer = k.optimizers.Adam(learning_rate=0.001, \n",
    "                              epsilon=1e-07)\n",
    "    train_loss_container = []\n",
    "    model = LSTMCNNModel(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)\n",
    "    EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4245.619140625\n",
      "Epoch 1 Batch 100 Loss 987.6618041992188\n",
      "Epoch 1 Loss 959.8344\n",
      "Time taken for 1 epoch 30.736896753311157 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2987.2880859375\n",
      "Epoch 2 Batch 100 Loss 374.241455078125\n",
      "Epoch 2 Loss 62.1179\n",
      "Time taken for 1 epoch 30.403167486190796 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 289.97589111328125\n",
      "Epoch 3 Batch 100 Loss 14.400070190429688\n",
      "Epoch 3 Loss 10.7125\n",
      "Time taken for 1 epoch 30.350748538970947 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 21.32500457763672\n",
      "Epoch 4 Batch 100 Loss 10.144912719726562\n",
      "Epoch 4 Loss 11.5718\n",
      "Time taken for 1 epoch 30.41493272781372 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 13.820745468139648\n",
      "Epoch 5 Batch 100 Loss 9.76859188079834\n",
      "Epoch 5 Loss 10.0829\n",
      "Time taken for 1 epoch 30.4047269821167 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.device('gpu'):\n",
    "    # EPOCHS = 10\n",
    "    for epoch in range(EPOCHS):\n",
    "        start_time = time.time()\n",
    "        if epoch > 0:\n",
    "            hidden = model.reset_states()\n",
    "\n",
    "\n",
    "        for (batch_number, (input_data, target_data)) in enumerate(dataset):\n",
    "            loss = train_step(input_data, \n",
    "                              target_data, \n",
    "                              model, \n",
    "                              optimizer,\n",
    "                              train_loss_container)\n",
    "            if batch_number % 100 == 0:\n",
    "                template = 'Epoch {} Batch {} Loss {}'\n",
    "                print(template.format(epoch+1, batch_number, loss))\n",
    "        model.save_weights('models/' + model.name + str(EPOCHS) + '_' + str(seq_length))\n",
    "        print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "        print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start_time))\n",
    "model.save_weights('models/' + model.name + str(EPOCHS) + '_' + str(seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Matrix size-incompatible: In[0]: [64,4096], In[1]: [153600,784] [Op:MatMul]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-deedd2735d4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_string\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu\"test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_generate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-0c857b815197>\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(model, start_string, num_generate)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;31m# using a categorical distribution to predict the word returned by the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-a0107be37f06>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, labels)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mx_lstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mx_lstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_lstm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mx_fc_reduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_reduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_lstm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         x_fc_reduction = tf.reshape(x_fc_reduction, \n\u001b[1;32m     34\u001b[0m                                     shape=[x_fc_reduction.shape[0],\n",
      "\u001b[0;32m~/dev/tools/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_tensor_dense_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   6124\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6125\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6126\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6127\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6128\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtranspose_a\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Matrix size-incompatible: In[0]: [64,4096], In[1]: [153600,784] [Op:MatMul]"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"test\", num_generate=150))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 123.33333400000001,
   "position": {
    "height": "40px",
    "left": "769.167px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
